{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_step_LSTM_uhichang_6month",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1supM104GVwWzFoZ-AEgEQjT9JEoqh_oC",
      "authorship_tag": "ABX9TyPTyY0YU1Z13ZPaCpow5ptn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JS-Choi513/Predicting-the-Housing-Price-Index-in-Changwon-City-Using-the-LSTM-Model/blob/main/Multi_step_LSTM_uhichang_6month_lagacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDoFIoF8ly9u"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from google.colab import files\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "#!pip install pyyaml h5py\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_VuNrc12RhZ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9guNBf-pDvf"
      },
      "source": [
        "uploaded = files.upload() \n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn]))) \n",
        "data = pd.read_csv('/content/integrated_data.csv')\n",
        "data['new_Date'] = pd.to_datetime(data['date'])\n",
        "print(data.head())\n",
        "print('\\n')\n",
        "print(data.info())\n",
        "print('\\n')\n",
        "#첫번째 컬럼을 날짜형식 컬럼으로 변경 \n",
        "print(type(data['new_Date'][0]))\n",
        "data.drop('date', axis = 1, inplace=True)\n",
        "data.set_index('new_Date', inplace=True)\n",
        "print(data.head())\n",
        "print('\\n')\n",
        "print(data.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ_y4FQBcTcY"
      },
      "source": [
        "#data['house_loan'].plot()\n",
        "plt.title(\"household_loan\")\n",
        "plt.plot(data['house_loan'])\n",
        "plt.show()\n",
        "plt.title(\"consuming_price_idx\")\n",
        "plt.plot(data['consumer_price'])\n",
        "plt.show()\n",
        "plt.title(\"building_permitted_area\")\n",
        "plt.plot(data['building_space'])\n",
        "plt.show()\n",
        "plt.title(\"Interest_rate\")\n",
        "plt.plot(data['interest_idx'])\n",
        "plt.show()\n",
        "plt.title(\"housing price_idx\")\n",
        "plt.plot(data['housing_price'])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZQzjctNuk-n"
      },
      "source": [
        "data.head()\n",
        "economic_stat = ['consumer_price','house_loan','building_space','interest_idx','housing_price']\n",
        "features = data[economic_stat]\n",
        "print(features.head(156))\n",
        "print(features.values)\n",
        "features.plot(subplots=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joSbHn7HxQ9V"
      },
      "source": [
        "#훈련데이터 분할 \n",
        "#2008-01-01 ~ 2018-01-01 120개월을훈련데이터로 분할 \n",
        "# 각 데이터 원시값-평균/표준편차 => 데이터셋 표준화 \n",
        "# 156 = \n",
        "TRAIN_SPLIT =120\n",
        "tf.random.set_seed(13)\n",
        "dataset = features.values\n",
        "data_mean = dataset[:TRAIN_SPLIT].mean(axis=0)#각 열에 대한120인덱스 까지의  평균값 \n",
        "data_std = dataset[:TRAIN_SPLIT].std(axis=0)# 각 열에 대한 표준편차차\n",
        "print(data_std)\n",
        "dataset = (dataset-data_mean)/data_std\n",
        "print(dataset)\n",
        "print(dataset.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoN7NEZm0xGS"
      },
      "source": [
        "#신경망 모델 훈련에 사용한 특정시간 윈도우 데이터 확보를 위한 메소드 \n",
        "#history: 과거데이터 크기\n",
        "#target size 모델이 미래를 예측할 크기 \n",
        "# step값이 주어지면 step에 맞게 데이터를 샘플링 \n",
        "\n",
        "def multivariate_data(dataset, target, start_index, end_index, history_size, target_size, step, single_step=False):\n",
        "    data = []          \n",
        "    labels = []\n",
        "    start_index = start_index + history_size\n",
        "    if end_index is None:\n",
        "        print(\"dataset len %d\",len(dataset))\n",
        "        end_index = len(dataset) - target_size\n",
        "    for i in range(start_index, end_index+1):\n",
        "        indices = range(i - history_size, i, step) \n",
        "        print(indices)\n",
        "        data.append(dataset[indices])#\n",
        "  \n",
        "        if single_step:\n",
        "          labels.append(target[i + target_size])\n",
        "          print(label)\n",
        "          print('타겟 데이터')\n",
        "          print(target[i + target_size])\n",
        "        else:\n",
        "          labels.append(target[i:i + target_size])\n",
        "                \n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "'''\n",
        "def multivariate_data(dataset, target, start_index, end_index, history_size, target_size, step, single_step=False):\n",
        "    data = []          \n",
        "    labels = []\n",
        "    start_index = start_index + history_size\n",
        "    count = 26\n",
        "    if end_index is None:\n",
        "        print(\"dataset len %d\",len(dataset))\n",
        "        end_index = len(dataset) - target_size # -target size는 마지막에 라벨데이터로 쓸 데이터가 없기때문에 사용\n",
        "        \n",
        "    for i in range(start_index, end_index+1):\n",
        "        indices = range(i - history_size, i, step) \n",
        "        \n",
        "        if count < 125:# 각 광역시별 마지막 슬라이딩 윈도우의 라벨값이 \n",
        "                      # 다음 광역시의 데이터로 들어가는것을 방지하기 위해\n",
        "                      # 151까지 슬라이딩 윈도우를 생성한 \n",
        "                      # 마지막 윈도우 (139 <= x < 151) +라벨 데이터: 151< y <=157 6개월 라벨 \n",
        "          print(indices)\n",
        "          data.append(dataset[indices])#\n",
        "          labels.append(target[i:i + target_size])\n",
        "        if count > 180: # 광역시 데이터가 겹치는 구간을 버리기 위함 슬라이딩 윈도우 끝부분이 150 이후부터 라벨데이터가 다음 광역시 값으로 들어감 따라서 167\n",
        "          count = 26    # 따라서 슬라이딩 윈도우의 시작이 다음 광역시의 첫 부분이 될 때 까지 버림 \n",
        "        count=count+1\n",
        "        \n",
        "    return np.array(data), np.array(labels)\n",
        "     '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACAPbNiqse_B"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhc8mzmg3bb_"
      },
      "source": [
        "past_history = 26\n",
        "\n",
        "# 13이나 39윈도우를 구성하면 각 광역시 마다독립적으로 윈도우 구성가능 \n",
        "# 2008-01-01 ~ 2019-12-01  \n",
        "# 13년치 *12 = 156행 * 광역시 7개 = 1092개 데이터 \n",
        "# 서울 부산 인천 대구 광주 대전 울산 \n",
        "# 156 약수 1 2 3 4 6 12 13 26 39 52 78 156\n",
        "#\n",
        "future_target = 6\n",
        "STEP = 1\n",
        "x_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 4], 0, TRAIN_SPLIT, past_history, future_target, STEP)#0 ~ 30000\n",
        "print('-----------------------------------')\n",
        "\n",
        "# 예측해야할 데이터셋  11개 윈도우 120 ~ 154\n",
        "x_val_multi, y_val_multi = multivariate_data(dataset, dataset[:, 4], TRAIN_SPLIT, None, past_history, future_target, STEP)#30000 ~ end\n",
        "\n",
        "print('Single window of past history : {}'.format(x_train_multi[0].shape))\n",
        "print(y_train_multi)\n",
        "print('================================================================')\n",
        "print('\\n Target temperature to predict : {}'.format(y_val_multi[0].shape))\n",
        "print(x_val_multi)\n",
        "print('y')\n",
        "print(y_val_multi)\n",
        "# 처음부터 약2년까지의  3개월 단위 표준화 데이터셋 5개의 컬럼, 8개 행.\n",
        "\n",
        "# 훈련데이터 차원 130,8,5..?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU9WeUxEffF8"
      },
      "source": [
        "base_model = tf.keras.models.load_model('/content/drive/MyDrive/6month_transfer_model2')\n",
        "base_model.summary()\n",
        "base_model.trainable = True\n",
        "plot_model(base_model,to_file='/content/model.png',show_shapes=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q25eFfDG5pAO"
      },
      "source": [
        "BATCH_SIZE = 256##144 #128#256\n",
        "#Batch는 일괄 처리되는 작업의 양이다. 위에서 설명된 data_size를 한번에 처리하는 갯수를 의미하며, \n",
        "#batch크기에 의해 weight 변화가 일어나며 batch크기 단위로 data loading 함수도 구현이 가능하다. \n",
        "#주의할점은 data_size/batch_size일때 나머지가 없어야 한다.\n",
        "tf.random.set_seed(13)\n",
        "np.random.seed(13)\n",
        "BUFFER_SIZE =  4096#데이터 14000\n",
        "EVALUATION_INTERVAL = 28\n",
        "EPOCHS = 720\n",
        "# 410 0.00005\n",
        "# 28/28 loss: 0.0022 - val_loss: 0.0359 700 -> 0.000001\n",
        "# 입력된 텐서로부터 slices를 생성한다. \n",
        "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
        "# 입력된 텐서로부터 slices를 생성한다. 무기한 반복\n",
        "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "# 데이터 캐싱, 훈련데이터 셔플 \n",
        "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi)) #11,9,5 11,0\n",
        "\n",
        "\n",
        "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()\n",
        "print(x_train_multi.shape)\n",
        "print(x_val_multi.shape)\n",
        "print(\"----------\")\n",
        "print(x_train_multi.shape[-2:])\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "  return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
        "\n",
        "\n",
        "base_model = tf.keras.models.load_model('/content/drive/MyDrive/6month_transfer_model')\n",
        "base_model.summary()\n",
        "base_model.trainable = True\n",
        "plot_model(base_model,to_file='/content/model.png',show_shapes=True)\n",
        "\n",
        "multi_step_model = tf.keras.models.Sequential()\n",
        "multi_step_model.add(tf.keras.layers.Dense(6,input_shape=base_model.output_shape[1:]))\n",
        "multi_step_model.add(tf.keras.layers.Dense(6))\n",
        "base_model.add(multi_step_model)\n",
        "base_model.summary()\n",
        "\n",
        "\n",
        "plot_model(base_model,to_file='/content/model2.png',show_shapes=True)\n",
        "base_model.compile(optimizer=tf.keras.optimizers.Adam(0.000003), loss = 'mse')# 에러 절댓값 평균ingle_step_model.add(tf.keras.layers.Dense(1))\n",
        "        #0.000005 0.0\\360 580\n",
        "import datetime\n",
        "log_dir = \"/content/log\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "multi_step_history = base_model.fit(train_data_multi, epochs=EPOCHS,\n",
        "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                                          validation_data=val_data_multi,\n",
        "                                          validation_steps=1)\n",
        "#steps_per_epoch [트레이닝데이터수/배치사이즈]\n",
        "#validation_steps validation data수/배치사이즈\n",
        "base_model.summary()\n",
        "\n",
        " #0s 7ms/step - loss: 0.0069 - val_loss: 0.0349\n",
        "print(multi_step_history)\n",
        "def plot_train_history(history, title):\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(len(loss))\n",
        "    \n",
        "    plt.figure()\n",
        "\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_train_history(multi_step_history,\n",
        "                   'Multi Step Training and Validation Loss')\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {log_dir}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hni6Kz8Pxv8"
      },
      "source": [
        "from keras.models import load_model\n",
        "base_model.save('6month_changwon_model_final.h5')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bVCbn8v7Kb2"
      },
      "source": [
        "def create_time_steps(length):\n",
        "    return list(range(-length, 0))\n",
        "\n",
        "\n",
        "def multi_step_plot(history, true_future, prediction):\n",
        "  plt.figure(figsize=(13,6))\n",
        "  num_in = create_time_steps(len(history))\n",
        "  num_out = len(true_future)\n",
        "  plt.plot(num_in, np.array(history[:,4]), label='History')\n",
        "  plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
        "           label='True Future')\n",
        "  if prediction.any():\n",
        "    plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',label='Predictied Future')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "count =0\n",
        "for x,y in val_data_multi.take(3):\n",
        "#예측에 필요한 데이터 (26,5), 다음 6개월치 데이터 반환 \n",
        "  multi_step_plot(x[count],y[count], base_model.predict(x)[count])\n",
        "  k = x[0]\n",
        "  count+=1\n",
        "  #print(base_model.predict(k))\n",
        "  print(x[0].shape)\n",
        "  print(x.shape)\n",
        "  print(x[0])\n",
        "  print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GtoPifEG8c_"
      },
      "source": [
        "()\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.title(\"Pridicting All time Housing price\")\n",
        "plt.plot(data['housing_price'])\n",
        "pridict = []\n",
        "for x,y in train_data_multi.take(1):\n",
        "  print(multi_step_model.predict(x).shape)\n",
        "  print(multi_step_model.predict(x))\n",
        "  #for window in range(96):\n",
        "  #plt.show()\n",
        "  print(x.shape)\n",
        "print(\"---------원본데이터-------------------------------\") \n",
        "print(data.shape)\n",
        "print(data[:,4])\n",
        "\n",
        "#  if prediction.any():\n",
        "\n",
        "#    plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',label='Predictied Future')\n",
        "#plt.legend(loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EGA9zzRUMrY"
      },
      "source": [
        "#모델 저장\n",
        "#!mkdir -p saved_model\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "notebooks_dir_name = 'notebooks'\n",
        "drive.mount('/content/gdrive')\n",
        "notebooks_basee_dir = path.join('./gdrive')\n",
        "multi_step_model.save('/content/saved_model/large_modle')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}